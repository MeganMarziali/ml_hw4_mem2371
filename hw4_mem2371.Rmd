---
title: "Assignment 4"
author: "Megan Marziali"
date: "Feb 3, 2021"
output:
  github_document: default
editor_options:
  chunk_output_type: console
---

## Part 1: Implementing a Simple Prediction Pipeline

### Problem set-up

##### Loading required paackages

The following code chunk loads the required packages for the assignment.
```{r packages, message = FALSE}
library(tidyverse)
library(Amelia)
library(caret)
library(stats)
library(factoextra)
library(cluster)
```

##### Loading data into environment and cleaning

```{r data_prepare, message = FALSE}
nyc_data = 
  read.csv("./data/class4_p1.csv", na = c("", ".", "NA", ".d", ".r")) %>% 
  janitor::clean_names() %>%
  mutate(
    bmi = as.numeric(bmi),
    gpaq8totmin = as.numeric(gpaq8totmin),
    gpaq11days = as.integer(gpaq11days),
    healthydays = as.integer(healthydays)
  )

# Reorganize all character to factor
# .d and .r are listed as missing

summary(nyc_data)
nrow(nyc_data)

missmap(nyc_data, main = "Missing values vs observed")
```

Based on missingness, I would not include the variables "habits7" and "povertygroup" in any models. I would also remove all missingness from variables that I intend to keep in models. With all variables and missing observations included, the total N for the dataset is 3811.

```{r, message = FALSE}
nyc_restr = 
  select(nyc_data, -habits7, -povertygroup) %>% 
  na.omit()

#nyc_restr = nyc_restr %>% distinct(X1, .keep_all = TRUE)

nrow(nyc_restr)
```

After removing the variables "habits7" (% missing) and "povertygroup" (% missing), and deleting missing observations, the total N for the dataset is 3552.

##### Data partitioning

```{r partition, message = FALSE}
set.seed(100)

train.indices = createDataPartition(y = nyc_restr$healthydays,p = 0.7,list = FALSE)

training = nyc_restr[train.indices,]
testing = nyc_restr[-train.indices,]
```

### Problem 1:

Fit two prediction  models using  different subsets of the features in the training data. Features can overlap in the two models, but the feature sets should not be exactly the same across models. Clearly state which features were used in the two models.

```{r models, message = FALSE}
model_1 <- lm(healthydays ~ chronic1 + chronic3 + chronic4 + bmi + gpaq8totmin + gpaq11days + habits5 + dem3, data = training)
summary(model_1)

model_2 <- lm(healthydays ~ tobacco1 + alcohol1 + bmi + gpaq8totmin + gpaq11days + habits5 + dem3, data = training)
summary(model_2)
```

### Problem 2:

Apply both models within the test data and determine which model is the preferred prediction model using the appropriate evaluation metric(s). 

```{r predictvalues, message = FALSE}
model_1_test = lm(healthydays ~ chronic1 + chronic3 + chronic4 + bmi + gpaq8totmin + gpaq11days + habits5 + dem3, data = testing)
summary(model_1_test)

model_2_test = lm(healthydays ~ tobacco1 + alcohol1 + bmi + gpaq8totmin + gpaq11days + habits5 + dem3, data = testing)
summary(model_2_test)

mse_1 = sum(model_1_test$residuals^2)
mse_2 = sum(model_2_test$residuals^2)

#Email TAs, ask about calibration in linear regression
```

The outcome we are investigating is continuous, so the appropriate evaluation metric is to assess mean squared error. As model 1 has the smaller mean squared error, we can determine that model 1 is the preferred prediction model.

### Problem 3

The implementation of this model would be helpful if in cases where we are trying to identify factors to intervene on to increase the number of healthy days experienced. As the model works to identify predictors of a higher number of healthy days, we know that those predictors function to increase healthy days experienced.

## Part II

### Problem set-up

```{r dataprep2}
data("USArrests")
head(USArrests)

#Removing missing
us_arrest = na.omit(USArrests)

colMeans(us_arrest, na.rm = TRUE)
apply(us_arrest, 2, sd, na.rm = TRUE)

# Data should be scaled???

#scale(us_arrest, center = TRUE, scale = TRUE)
```

I have opted not to scale the data because they are all in the same units.

We can use the kmeans function in order to identify clusters within the data, based on the three variables.
```{r}
set.seed(100)
clusters = kmeans(us_arrest, 5, nstart = 25)
str(clusters)
fviz_cluster(clusters, data = us_arrest)
#Show the mean value of features within each cluster
clusters$centers

#Conduct a gap_statistic analysis to determine optimal number of clusters
set.seed(100)
gap_stat = clusGap(us_arrest, FUN = kmeans, nstart = 30, K.max = 24, B = 50)
print(gap_stat, method = "firstmax")

#How do we know what our max k should be?

set.seed(100)
gap_stat <- clusGap(us_arrest, FUN = kmeans, nstart = 30, K.max = 24, B = 50)
fviz_gap_stat(gap_stat) + theme_minimal() + ggtitle("fviz_gap_stat: Gap Statistic")

clusters.6 = kmeans(us_arrest, 6, nstart = 25)
str(clusters.6)
fviz_cluster(clusters.6, data = us_arrest)
```

DELETE ABOVE CODE.

```{r}
# Create Dissimilarity matrix
diss.matrix = dist(us_arrest, method = "euclidean")

# Hierarchical clustering using COMPLETE Linkage
clusters.h = hclust(diss.matrix, method = "complete" )

# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)

#create function to use within clusGap
hclusCut = function(x, k) list(cluster = cutree(hclust(dist(x, method = "euclidian"), method = "complete"), k = k))

gap_stat = clusGap(us_arrest, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

# Hierarchical clustering using AVERAGE Linkage
clusters.a = hclust(diss.matrix, method = "average" )

hclusCut = function(x, k) list(cluster = cutree(hclust(dist(x, method = "euclidian"), method = "average"), k = k))

gap_stat = clusGap(us_arrest, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

#####do i need to be setting a seed???

#Use number of clusters from gap statistic to obtain cluster assignment for each observation
clusters.h.3 = cutree(clusters.h, k = 3)
table(clusters.h.3)

clusters.a.3 = cutree(clusters.a, k = 3)
table(clusters.a.3)

clusters.3 = kmeans(us_arrest, 3, nstart = 25)
str(clusters.3)
fviz_cluster(clusters.3, data = us_arrest)
```

